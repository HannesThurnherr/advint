program: train_sae_adv_kl.py # The script to run
method: bayes # Or random, grid
metric:
  name: train/loss_llm # The metric to optimize (based on wandb logging keys)
  goal: minimize        # Minimize this metric
parameters:
  entropy_lambda:
    # Sample values logarithmically between 0.01 and 2.0 (adjust range as needed)
    distribution: log_uniform_values
    min: 0.01
    max: 10
  # --- Add other parameters you want constant across the sweep ---
  # --- Or define distributions for them if sweeping multiple params ---
  llm_lr:
    value: 5e-5
  sae_lr:
    value: 5e-4
  sae_lambda:
    value: 0.2 # Keep fixed as it's not directly used in the swept loss term
  batch_size:
    value: 8
  max_batches:
    value: 7500 # Or adjust as needed for sweep runs
  val_every:
    value: 200 # Match the updated default in the script
  val_batches:
    value: 100
  layer_num:
    value: 3
  sae_mul:
    value: 10
  k:
    value: 25
  wandb_log_freq:
     value: 10 # Match the updated default in the script
  # --- Specify paths if they are fixed for the sweep ---
  # llm_in:
  #   value: "models/lm_adv.pth" # Optional: Start from a specific LLM
  # sae_in:
  #   value: "models/sae_base_e2e.pth" # Optional: Start from a specific SAE
  llm_out:
    value: "models/sweep_lm/lm_adv_sweep.pth" # Use a dedicated sweep output dir
  sae_out:
    value: "models/sweep_sae/sae_adv_sweep.pth" # Use a dedicated sweep output dir
  # --- Wandb config ---
  wandb_project:
    value: "sae_adv_entropy_sweep" # Specific project for this sweep
  # wandb_entity: # Optional: Add your entity here if needed
  #   value: "your_wandb_username_or_team"